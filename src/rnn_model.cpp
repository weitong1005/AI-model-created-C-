#include "rnn_model.h"
#include <fstream>
#include <cmath>
#include <iostream>
#include <random>
#include <stdexcept>

// æ·»åŠ NaNæ£€æŸ¥å‡½æ•°
bool isNaN(double x) {
    return x != x;
}

// æ·»åŠ æ¢¯åº¦è£å‰ªå‡½æ•°
void clipGradients(Matrix& grad, double maxNorm) {
    double norm = 0.0;
    for (int i = 0; i < grad.rows; i++) {
        for (int j = 0; j < grad.cols; j++) {
            norm += grad.data[i][j] * grad.data[i][j];
        }
    }
    norm = sqrt(norm);

    if (norm > maxNorm && norm > 0) {
        double scale = maxNorm / norm;
        grad *= scale;
    }
}

RNNModel::RNNModel(int vocabSize, int hiddenSize, int layers)
        : vocabSize(vocabSize), hiddenSize(hiddenSize), layers(layers), activationType(TANH) {
    // æ”¹è¿›æƒé‡åˆå§‹åŒ–ï¼šä½¿ç”¨Xavieråˆå§‹åŒ–ï¼Œè§£å†³åˆå§‹å€¼è¿‡å¤§é—®é¢˜
    double wxhScale = sqrt(1.0 / vocabSize);
    Wxh = Matrix(hiddenSize, vocabSize, true);
    Wxh *= wxhScale;  // æ ¹æ®è¾“å…¥ç»´åº¦ç¼©æ”¾

    Whh.resize(layers);
    double whhScale = sqrt(1.0 / hiddenSize);
    for (int i = 0; i < layers; i++) {
        Whh[i] = Matrix(hiddenSize, hiddenSize, true);
        Whh[i] *= whhScale;  // æ ¹æ®éšè—å±‚ç»´åº¦ç¼©æ”¾
    }

    double whyScale = sqrt(1.0 / hiddenSize);
    Why = Matrix(vocabSize, hiddenSize, true);
    Why *= whyScale;

    // åç½®åˆå§‹åŒ–ä¸ºè¾ƒå°çš„å¸¸æ•°ï¼Œé¿å…åˆå§‹ä¸º0
    bh.resize(layers, Matrix(hiddenSize, 1));
    for (int i = 0; i < layers; i++) {
        for (int j = 0; j < hiddenSize; j++) {
            bh[i].data[j][0] = 0.01;  // å°çš„æ­£å€¼åˆå§‹åŒ–
        }
    }

    by = Matrix(vocabSize, 1);
    for (int i = 0; i < vocabSize; i++) {
        by.data[i][0] = 0.01;
    }

    // åˆå§‹åŒ–Adamä¼˜åŒ–å™¨å‚æ•°
    optWxh.m = Matrix(hiddenSize, vocabSize);
    optWxh.v = Matrix(hiddenSize, vocabSize);
    optWhy.m = Matrix(vocabSize, hiddenSize);
    optWhy.v = Matrix(vocabSize, hiddenSize);

    optWhh.resize(layers);
    optBh.resize(layers);
    for (int i = 0; i < layers; i++) {
        optWhh[i].m = Matrix(hiddenSize, hiddenSize);
        optWhh[i].v = Matrix(hiddenSize, hiddenSize);
        optBh[i].m = Matrix(hiddenSize, 1);
        optBh[i].v = Matrix(hiddenSize, 1);
    }

    optBy.m = Matrix(vocabSize, 1);
    optBy.v = Matrix(vocabSize, 1);
}

Matrix RNNModel::activate(const Matrix& m) {
    // æ¿€æ´»å‡½æ•°è¾“å‡ºåæ£€æŸ¥å¹¶å¤„ç†NaN
    Matrix result = Matrix::leakyRelu(m);  // Leaky ReLUæ›´ä¸å®¹æ˜“å‡ºç°æ­»äº¡ç¥ç»å…ƒ

    // æ£€æŸ¥å¹¶æ›¿æ¢NaNå€¼
    for (int i = 0; i < result.rows; i++) {
        for (int j = 0; j < result.cols; j++) {
            if (isNaN(result.data[i][j])) {
                result.data[i][j] = 0.01;  // ç”¨å°å€¼æ›¿æ¢NaN
            }
        }
    }

    return result;
}

Matrix RNNModel::activateDerivative(const Matrix& m) {
    return Matrix::leakyReluDerivative(m);
}

// æ”¹è¿›çš„softmaxå‡½æ•°ï¼Œæ·»åŠ æ•°å€¼ç¨³å®šå¤„ç†
Matrix softmax(const Matrix& m) {
    Matrix result(m.rows, m.cols);
    double maxVal = m.data[0][0];

    // æ‰¾åˆ°æœ€å¤§å€¼ï¼Œç”¨äºæ•°å€¼ç¨³å®š
    for (int i = 0; i < m.rows; i++) {
        if (m.data[i][0] > maxVal) {
            maxVal = m.data[i][0];
        }
    }

    // è®¡ç®—æŒ‡æ•°å¹¶å‡å»æœ€å¤§å€¼é˜²æ­¢æº¢å‡º
    double sum = 0.0;
    for (int i = 0; i < m.rows; i++) {
        result.data[i][0] = exp(m.data[i][0] - maxVal);
        sum += result.data[i][0];
    }

    // å½’ä¸€åŒ–
    for (int i = 0; i < m.rows; i++) {
        result.data[i][0] /= sum;
        // å¤„ç†å¯èƒ½çš„NaNï¼ˆå½“sumä¸º0æ—¶ï¼‰
        if (isNaN(result.data[i][0])) {
            result.data[i][0] = 1.0 / m.rows;
        }
    }

    return result;
}

// ä¿®æ”¹forwardæ–¹æ³•ä¸­çš„çŸ©é˜µè¿ç®—éƒ¨åˆ†
RNNModel::ForwardCache RNNModel::forward(const std::vector<int>& inputs) {
    ForwardCache cache(layers, hiddenSize, vocabSize);

    for (size_t t = 0; t < inputs.size(); t++) {
        int idx = inputs[t];
        if (idx < 0 || idx >= vocabSize) {
            std::cerr << "æ— æ•ˆæ±‰å­—ç´¢å¼•: " << idx << "\n";
            std::exit(1);
        }

        // è¾“å…¥å±‚ç‹¬çƒ­ç¼–ç  [vocabSize x 1]
        Matrix x(vocabSize, 1);
        x.data[idx][0] = 1.0;

        // ç¬¬ä¸€å±‚è®¡ç®—ï¼šä¿®å¤çŸ©é˜µç»´åº¦ä¸åŒ¹é…
        // [hiddenSize x 1] = ([hiddenSize x vocabSize] * [vocabSize x 1]) +
        //                   ([hiddenSize x hiddenSize] * [hiddenSize x 1]) + [hiddenSize x 1]
        Matrix wxhResult = Wxh * x;          // [hiddenSize x 1]
        Matrix whhResult = Whh[0] * cache.h[0];  // [hiddenSize x 1]
        cache.a[0] = wxhResult + whhResult + bh[0];  // æ­£ç¡®çš„ç»´åº¦ç›¸åŠ 
        cache.h[0] = activate(cache.a[0]);

        // æ·±å±‚è®¡ç®—
        for (int l = 1; l < layers; l++) {
            // [hiddenSize x 1] = [hiddenSize x hiddenSize] * [hiddenSize x 1] + [hiddenSize x 1]
            cache.a[l] = Whh[l] * cache.h[l-1] + bh[l];
            cache.h[l] = activate(cache.a[l]);
        }

        // è¾“å‡ºå±‚è®¡ç®— [vocabSize x 1] = [vocabSize x hiddenSize] * [hiddenSize x 1] + [vocabSize x 1]
        cache.y = Why * cache.h[layers-1] + by;
    }

    return cache;
}

void RNNModel::adamUpdate(Matrix& param, AdamParams& opt, const Matrix& grad, double lr, int t) {
    // å¤åˆ¶æ¢¯åº¦ç”¨äºä¿®æ”¹
    Matrix safeGrad = grad;

    // æ£€æŸ¥å¹¶æ›¿æ¢æ¢¯åº¦ä¸­çš„NaN
    for (int i = 0; i < safeGrad.rows; i++) {
        for (int j = 0; j < safeGrad.cols; j++) {
            if (isNaN(safeGrad.data[i][j])) {
                safeGrad.data[i][j] = 0.0;  // NaNæ¢¯åº¦æ›¿æ¢ä¸º0
            }
        }
    }

    // æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    clipGradients(safeGrad, 5.0);  // æœ€å¤§èŒƒæ•°è®¾ä¸º5.0

    for (int i = 0; i < param.rows; i++) {
        for (int j = 0; j < param.cols; j++) {
            // æ›´æ–°ä¸€é˜¶çŸ©ä¼°è®¡
            opt.m.data[i][j] = opt.beta1 * opt.m.data[i][j] + (1 - opt.beta1) * safeGrad.data[i][j];
            // æ›´æ–°äºŒé˜¶çŸ©ä¼°è®¡
            opt.v.data[i][j] = opt.beta2 * opt.v.data[i][j] + (1 - opt.beta2) * safeGrad.data[i][j] * safeGrad.data[i][j];

            // åå·®ä¿®æ­£
            double m_hat = opt.m.data[i][j] / (1 - pow(opt.beta1, t));
            double v_hat = opt.v.data[i][j] / (1 - pow(opt.beta2, t));

            // å‚æ•°æ›´æ–°ï¼Œæ·»åŠ é¢å¤–ä¿æŠ¤é˜²æ­¢NaN
            if (!isNaN(m_hat) && !isNaN(v_hat) && v_hat > 0) {
                param.data[i][j] -= lr * m_hat / (sqrt(v_hat) + opt.epsilon);
            }

            // æœ€åæ£€æŸ¥å‚æ•°æ˜¯å¦ä¸ºNaN
            if (isNaN(param.data[i][j])) {
                param.data[i][j] = 0.01;  // é‡ç½®ä¸ºå°å€¼
            }
        }
    }
}

void RNNModel::train(const std::vector<int>& sequence, double learningRate, int epoch) {
    if (sequence.size() < 2) return;

    std::vector<int> inputs(sequence.begin(), sequence.end() - 1);
    std::vector<int> targets(sequence.begin() + 1, sequence.end());

    auto cache = forward(inputs);
    int lastTarget = targets.back();
    if (lastTarget < 0 || lastTarget >= vocabSize) return;

    // è¾“å‡ºè¯¯å·® [vocabSize x 1]
    Matrix dy(vocabSize, 1);
    for (int i = 0; i < vocabSize; i++) {
        dy.data[i][0] = cache.y.data[i][0] - (i == lastTarget ? 1.0 : 0.0);
    }

    // è®¡ç®—è¯¯å·®é¡¹
    std::vector<Matrix> dh(layers, Matrix(hiddenSize, 1));

    // æœ€åä¸€å±‚éšè—å±‚è¯¯å·®
    Matrix whyT = Why.transpose();  // [hiddenSize x vocabSize]
    dh[layers-1] = whyT * dy;       // [hiddenSize x 1] = [hiddenSize x vocabSize] * [vocabSize x 1]
    dh[layers-1] = dh[layers-1] * activateDerivative(cache.a[layers-1]);

    // æ·±å±‚è¯¯å·®åå‘ä¼ æ’­
    for (int l = layers-2; l >= 0; l--) {
        Matrix whhT = Whh[l+1].transpose();  // [hiddenSize x hiddenSize]
        dh[l] = whhT * dh[l+1];              // [hiddenSize x 1] = [hiddenSize x hiddenSize] * [hiddenSize x 1]
        dh[l] = dh[l] * activateDerivative(cache.a[l]);
    }

    // æ›´æ–°æƒé‡ï¼ˆç¡®ä¿æ‰€æœ‰çŸ©é˜µä¹˜æ³•ç»´åº¦æ­£ç¡®ï¼‰
    Matrix whyGrad = dy * cache.h[layers-1].transpose();  // [vocabSize x hiddenSize]
    adamUpdate(Why, optWhy, whyGrad, learningRate, epoch);

    // æ›´æ–°ç¬¬ä¸€å±‚æƒé‡
    Matrix x(vocabSize, 1);
    x.data[inputs.back()][0] = 1.0;
    Matrix wxhGrad = dh[0] * x.transpose();  // [hiddenSize x vocabSize]
    adamUpdate(Wxh, optWxh, wxhGrad, learningRate, epoch);

    // æ›´æ–°éšè—å±‚æƒé‡
    for (int l = 0; l < layers; l++) {
        Matrix hPrev = (l == 0) ? cache.h[0] : cache.h[l-1];
        Matrix whhGrad = dh[l] * hPrev.transpose();  // [hiddenSize x hiddenSize]
        adamUpdate(Whh[l], optWhh[l], whhGrad, learningRate, epoch);
        adamUpdate(bh[l], optBh[l], dh[l], learningRate, epoch);
    }

    adamUpdate(by, optBy, dy, learningRate, epoch);
}

int RNNModel::predict(const std::vector<int>& inputSeq, bool random, double temperature) {
    auto cache = forward(inputSeq);

    // åº”ç”¨æ¸©åº¦è°ƒæ•´æ¦‚ç‡åˆ†å¸ƒï¼Œä½¿ç”¨ç¨³å®šçš„softmax
    Matrix probs = softmax(cache.y);

    // æ¸©åº¦è°ƒæ•´
    for (int i = 0; i < vocabSize; i++) {
        if (probs.data[i][0] <= 0) {
            probs.data[i][0] = 1e-10;  // é˜²æ­¢log(0)
        }
        probs.data[i][0] = pow(probs.data[i][0], 1.0 / temperature);
    }

    // å½’ä¸€åŒ–æ¦‚ç‡
    double sum = 0.0;
    for (int i = 0; i < vocabSize; i++) {
        sum += probs.data[i][0];
    }

    // å¤„ç†sumä¸º0çš„æƒ…å†µ
    if (sum <= 0) {
        for (int i = 0; i < vocabSize; i++) {
            probs.data[i][0] = 1.0 / vocabSize;
        }
        sum = 1.0;
    }

    for (int i = 0; i < vocabSize; i++) {
        probs.data[i][0] /= sum;
        // æœ€ç»ˆæ£€æŸ¥NaN
        if (isNaN(probs.data[i][0])) {
            probs.data[i][0] = 1.0 / vocabSize;
        }
    }

    if (!random) {
        // é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„å­—ç¬¦
        int maxIdx = 0;
        double maxVal = probs.data[0][0];
        for (int i = 1; i < vocabSize; i++) {
            if (probs.data[i][0] > maxVal) {
                maxVal = probs.data[i][0];
                maxIdx = i;
            }
        }
        return maxIdx;
    } else {
        // åŸºäºæ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©
        std::random_device rd;
        std::mt19937 gen(rd());
        std::uniform_real_distribution<> dis(0.0, 1.0);
        double r = dis(gen);

        double accum = 0.0;
        for (int i = 0; i < vocabSize; i++) {
            accum += probs.data[i][0];
            if (accum >= r) {
                return i;
            }
        }
        return vocabSize - 1; // fallback
    }
}

// ä¿æŒå…¶ä»–å‡½æ•°ä¸å˜...
bool RNNModel::save(const std::string& filename) {
    std::ofstream fout(filename, std::ios::binary);
    if (!fout) return false;

    fout.write((char*)&vocabSize, sizeof(vocabSize));
    fout.write((char*)&hiddenSize, sizeof(hiddenSize));
    fout.write((char*)&layers, sizeof(layers));
    fout.write((char*)&activationType, sizeof(activationType));

    auto saveMatrix = [&](const Matrix& m) {
        fout.write((char*)&m.rows, sizeof(m.rows));
        fout.write((char*)&m.cols, sizeof(m.cols));
        for (const auto& row : m.data)
            fout.write((char*)row.data(), row.size() * sizeof(double));
    };

    saveMatrix(Wxh);
    for (const auto& m : Whh) saveMatrix(m);
    saveMatrix(Why);
    for (const auto& m : bh) saveMatrix(m);
    saveMatrix(by);

    return true;
}

bool RNNModel::load(const std::string& filename) {
    std::ifstream fin(filename, std::ios::binary);
    if (!fin) return false;

    fin.read((char*)&vocabSize, sizeof(vocabSize));
    fin.read((char*)&hiddenSize, sizeof(hiddenSize));
    fin.read((char*)&layers, sizeof(layers));

    int actType;
    fin.read((char*)&actType, sizeof(actType));
    activationType = static_cast<ActivationType>(actType);

    auto loadMatrix = [&](Matrix& m) {
        int rows, cols;
        fin.read((char*)&rows, sizeof(rows));
        fin.read((char*)&cols, sizeof(cols));
        m = Matrix(rows, cols);
        for (int i = 0; i < rows; i++) {
            m.data[i].resize(cols);
            fin.read((char*)m.data[i].data(), cols * sizeof(double));
        }
    };

    loadMatrix(Wxh);
    Whh.resize(layers);
    for (int i = 0; i < layers; i++) loadMatrix(Whh[i]);
    loadMatrix(Why);
    bh.resize(layers);
    for (int i = 0; i < layers; i++) loadMatrix(bh[i]);
    loadMatrix(by);

    // é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨å‚æ•°
    optWxh = AdamParams();
    optWhy = AdamParams();
    optWhh.resize(layers);
    optBh.resize(layers);
    optBy = AdamParams();

    return true;
}

void RNNModel::selfLearn(int iterations, int seqLen, double lr) {
    double currentLr = lr;
    int consecutiveErrors = 0;

    for (int i = 0; i < iterations; i++) {
        try {
            std::vector<int> seq;
            for (int j = 0; j < seqLen; j++) {
                seq.push_back(rand() % vocabSize);
            }

            // æ¯1000æ¬¡è¿­ä»£é™ä½ä¸€æ¬¡å­¦ä¹ ç‡
            if (i % 1000 == 0 && i > 0) {
                currentLr *= 0.95;  // 5%è¡°å‡ç‡
            }

            train(seq, currentLr, i);
            consecutiveErrors = 0;  // é‡ç½®é”™è¯¯è®¡æ•°

            // æ¯5000æ¬¡è¿­ä»£è¾“å‡ºè¿›åº¦
            if (i % 5000 == 0) {
                printf("ğŸ“Š è®­ç»ƒè¿›åº¦: %.1f%% (è¿­ä»£ %d/%d)\n",
                       (double)i/iterations*100, i, iterations);
            }
        } catch (const std::exception& e) {
            std::cerr << "âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: " << e.what() << "\n";
            consecutiveErrors++;

            // å¦‚æœè¿ç»­å‡ºé”™ï¼Œé™ä½å­¦ä¹ ç‡
            if (consecutiveErrors >= 5) {
                currentLr *= 0.5;
                std::cerr << "âš ï¸ è¿ç»­é”™è¯¯ï¼Œé™ä½å­¦ä¹ ç‡è‡³: " << currentLr << "\n";
                consecutiveErrors = 0;

                // å¦‚æœå­¦ä¹ ç‡è¿‡å°ï¼Œä»ç„¶å‡ºé”™ï¼Œåˆ™ç»ˆæ­¢è®­ç»ƒ
                if (currentLr < 1e-8) {
                    throw std::runtime_error("å­¦ä¹ ç‡å·²è¿‡å°ä½†ä»å‡ºé”™ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ");
                }
            }
        }
    }
}



////
//// Created by bm on 25-7-23.
////
//#include "rnn_model.h"
//#include <fstream>
//#include <cmath>
//#include <iostream>
//
//RNNModel::RNNModel(int vocabSize, int hiddenSize, int layers)
//        : vocabSize(vocabSize), hiddenSize(hiddenSize), layers(layers) {
//    // åˆå§‹åŒ–æƒé‡çŸ©é˜µï¼ˆç¡®ä¿ç»´åº¦æ­£ç¡®ï¼‰
//    Wxh = Matrix(hiddenSize, vocabSize, true);  // è¾“å…¥â†’éšè—: [h x v]
//    Whh.resize(layers, Matrix(hiddenSize, hiddenSize, true));  // éšè—â†’éšè—: [h x h]
//    Why = Matrix(vocabSize, hiddenSize, true);  // éšè—â†’è¾“å‡º: [v x h]
//    bh.resize(layers, Matrix(hiddenSize, 1, true));  // éšè—å±‚åç½®: [h x 1]
//    by = Matrix(vocabSize, 1, true);  // è¾“å‡ºå±‚åç½®: [v x 1]
//}
//
//RNNModel::ForwardCache RNNModel::forward(const std::vector<int>& inputs) {
//    ForwardCache cache(layers, hiddenSize, vocabSize);
//
//    for (size_t t = 0; t < inputs.size(); t++) {
//        // è¾“å…¥å±‚ç‹¬çƒ­ç¼–ç  [v x 1]
//        int idx = inputs[t];
//        if (idx < 0 || idx >= vocabSize) {
//            std::cerr << "æ— æ•ˆæ±‰å­—ç´¢å¼•: " << idx << "\n";
//            std::exit(1);
//        }
//        Matrix x(vocabSize, 1);
//        x.data[idx][0] = 1.0;
//
//        // ç¬¬ä¸€å±‚è®¡ç®— [h x 1] = [h x v] * [v x 1] + [h x h] * [h x 1] + [h x 1]
//        Matrix h1 = Wxh * x + Whh[0] * cache.h[0] + bh[0];
//        cache.h[0] = Matrix::tanh(h1);
//
//        // æ·±å±‚è®¡ç®—ï¼ˆä»…1å±‚æ—¶ä¸æ‰§è¡Œï¼‰
//        for (int l = 1; l < layers; l++) {
//            Matrix hl = Whh[l] * cache.h[l-1] + bh[l];
//            cache.h[l] = Matrix::tanh(hl);
//        }
//
//        // è¾“å‡ºå±‚è®¡ç®— [v x 1] = [v x h] * [h x 1] + [v x 1]
//        cache.y = Why * cache.h[layers-1] + by;
//    }
//    return cache;
//}
//
//void RNNModel::train(const std::vector<int>& sequence, double learningRate) {
//    if (sequence.size() < 2) return;
//    std::vector<int> inputs(sequence.begin(), sequence.end() - 1);
//    std::vector<int> targets(sequence.begin() + 1, sequence.end());
//
//    auto cache = forward(inputs);
//    int lastTarget = targets.back();
//    if (lastTarget < 0 || lastTarget >= vocabSize) return;
//
//    // è¾“å‡ºè¯¯å·® [v x 1]
//    Matrix dy(vocabSize, 1);
//    for (int i = 0; i < vocabSize; i++) {
//        dy.data[i][0] = cache.y.data[i][0] - (i == lastTarget ? 1.0 : 0.0);
//    }
//
//    // æœ€åä¸€å±‚éšè—å±‚è¯¯å·® [h x 1] = [h x v] * [v x 1]ï¼ˆWhyè½¬ç½®åæ˜¯[h x v]ï¼‰
//    std::vector<Matrix> dh(layers);
//    dh[layers-1] = Why.transpose() * dy;
//    dh[layers-1] = dh[layers-1] * Matrix::tanhDerivative(cache.h[layers-1]);
//
//    // æ·±å±‚è¯¯å·®åå‘ä¼ æ’­ï¼ˆä»…1å±‚æ—¶ä¸æ‰§è¡Œï¼‰
//    for (int l = layers-2; l >= 0; l--) {
//        dh[l] = Whh[l+1].transpose() * dh[l+1];
//        dh[l] = dh[l] * Matrix::tanhDerivative(cache.h[l]);
//    }
//
//    // æ›´æ–°è¾“å‡ºå±‚æƒé‡ [v x h] = [v x h] - [v x 1] * [1 x h] * lr
//    Matrix why_grad = dy * cache.h[layers-1].transpose();  // å…³é”®ä¿®å¤ï¼šä½¿ç”¨è½¬ç½®
//    Why = Why - why_grad * learningRate;
//    by = by - dy * learningRate;
//
//    // æ›´æ–°ç¬¬ä¸€å±‚è¾“å…¥æƒé‡ [h x v] = [h x v] - [h x 1] * [1 x v] * lr
//    int lastInput = inputs.back();
//    Matrix x(vocabSize, 1);
//    x.data[lastInput][0] = 1.0;
//    Matrix wxh_grad = dh[0] * x.transpose();  // å…³é”®ä¿®å¤ï¼šä½¿ç”¨è½¬ç½®
//    Wxh = Wxh - wxh_grad * learningRate;
//
//    // æ›´æ–°éšè—å±‚æƒé‡ [h x h] = [h x h] - [h x 1] * [1 x h] * lr
//    for (int l = 0; l < layers; l++) {
//        Matrix h_prev = (l == 0) ? cache.h[0] : cache.h[l-1];
//        Matrix whh_grad = dh[l] * h_prev.transpose();  // å…³é”®ä¿®å¤ï¼šä½¿ç”¨è½¬ç½®
//        Whh[l] = Whh[l] - whh_grad * learningRate;
//        bh[l] = bh[l] - dh[l] * learningRate;
//    }
//}
//
//int RNNModel::predict(const std::vector<int>& inputSeq, bool random) {
//    auto cache = forward(inputSeq);
//    if (random) {
//        double sum = 0;
//        for (int i = 0; i < vocabSize; i++)
//            sum += exp(cache.y.data[i][0]);
//        double r = (double)rand() / RAND_MAX;
//        double accum = 0;
//        for (int i = 0; i < vocabSize; i++) {
//            accum += exp(cache.y.data[i][0]) / sum;
//            if (accum >= r) return i;
//        }
//    }
//    int maxIdx = 0;
//    double maxVal = cache.y.data[0][0];
//    for (int i = 1; i < vocabSize; i++) {
//        if (cache.y.data[i][0] > maxVal) {
//            maxVal = cache.y.data[i][0];
//            maxIdx = i;
//        }
//    }
//    return maxIdx;
//}
//
//void RNNModel::selfLearn(int iterations, int seqLen, double lr) {
//    for (int i = 0; i < iterations; i++) {
//        std::vector<int> seq;
//        for (int j = 0; j < seqLen; j++)
//            seq.push_back(rand() % vocabSize);
//        train(seq, lr);
//        if (i % 5000 == 0)
//            printf("è¿­ä»£ %d/%d\n", i, iterations);
//    }
//}
//
//bool RNNModel::save(const std::string& filename) {
//    std::ofstream fout(filename, std::ios::binary);
//    if (!fout) return false;
//
//    fout.write((char*)&vocabSize, sizeof(vocabSize));
//    fout.write((char*)&hiddenSize, sizeof(hiddenSize));
//    fout.write((char*)&layers, sizeof(layers));
//
//    auto saveMatrix = [&](const Matrix& m) {
//        fout.write((char*)&m.rows, sizeof(m.rows));
//        fout.write((char*)&m.cols, sizeof(m.cols));
//        for (const auto& row : m.data)
//            fout.write((char*)row.data(), row.size() * sizeof(double));
//    };
//
//    saveMatrix(Wxh);
//    for (const auto& m : Whh) saveMatrix(m);
//    saveMatrix(Why);
//    for (const auto& m : bh) saveMatrix(m);
//    saveMatrix(by);
//
//    return true;
//}
//
//bool RNNModel::load(const std::string& filename) {
//    std::ifstream fin(filename, std::ios::binary);
//    if (!fin) return false;
//
//    fin.read((char*)&vocabSize, sizeof(vocabSize));
//    fin.read((char*)&hiddenSize, sizeof(hiddenSize));
//    fin.read((char*)&layers, sizeof(layers));
//
//    auto loadMatrix = [&](Matrix& m) {
//        int rows, cols;
//        fin.read((char*)&rows, sizeof(rows));
//        fin.read((char*)&cols, sizeof(cols));
//        m = Matrix(rows, cols);
//        for (int i = 0; i < rows; i++) {
//            m.data[i].resize(cols);
//            fin.read((char*)m.data[i].data(), cols * sizeof(double));
//        }
//    };
//
//    loadMatrix(Wxh);
//    Whh.resize(layers);
//    for (int i = 0; i < layers; i++) loadMatrix(Whh[i]);
//    loadMatrix(Why);
//    bh.resize(layers);
//    for (int i = 0; i < layers; i++) loadMatrix(bh[i]);
//    loadMatrix(by);
//
//    return true;
//}
